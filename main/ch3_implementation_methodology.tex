\chapter{Implementation methodology}
As stated before we want to evaluate the perforamnce of CNN's to reconstruct images from compressed measurements. In this chapter we will describe the tools and software frameworks used in order to build the set-up for our experiments. First,  we introduce some of the most widly used libraries to build and train neural networks and the specific software tools for this thesis. Second, we briefly describe Graphics Processing Units (GPU's) and give reasons why they are an important tool when working with deep learning. Third, we will describe the datasets for training the networks and briefly justify its utilization. Then, we explain how the data is pre and postprocessed. This is important since it allows to efficiently use the hardware resources and makes the training process numerically stable. Finally, we propose CNN architectures for the reconstrution process.   

\section{Theano}
Theano is a library written in python that permits to define, optimize and compute mathematical expressions that deal with high-dimensional arrays in an efficient way. It is widly used among the deep learning community because it is optmized to make use of GPU routines that make training faster, efficient symbolic differentiation, stability optimizations and therefore making it realiable since it is constantly tested and debugged \cite{2016arXiv160502688short}. Nevertheless, Theano is not intendet to be used only for neural natwork applications. Therefore, it is necessary to write routines or API's that specially handle the difficulties encountered for deep learning. Such applications are normally an extra abstraction layer that sits on top of Theano's implementation and specially allow to build and train neural networks. Examples publically avaible are Lasagne \cite{sander_dieleman_2015_27878}, Blocks \cite{van2015blocks} and Keras \cite{chollet2015keras}. \
Other common frameworks in deep learning are Google's Tensorflow \cite{tensorflow2015-whitepaper} that is written in C++, Torch \cite{torch} written in Lua and Caffe \cite{jia2014caffe} also written in C++. Most of them offer the same characteristics and mostly differ in the language they are written in, speed and target application, for example Caffe is not suitable for audio and text applciations.           
\subsection{Sdeepy}
Due to the fact that the previously mentioned implementations are constanly updated and changing, Sony decided to develop its own library to develop its projects. Sdeepy is an in-house Sony's deep learning library implmentation that makes used of Theano and incorporates routines commonly used as building-blocks for training and testing neural networks. It has the advantage that many new features can be added at any time they become available while maintaining the stability. Furthermore, it is easier to adapt and modify according to one's needs. Because of all that, we wil use Sdeepy for this thesis but, all implementations might be easily translated into any available framework.
\section{Graphics Procesing Unit (GPU)}
\begin{figure}[tb] 
\centering 
\includegraphics[scale=0.6]{GPU1.png} 
\caption[GPU for deep learning]{NVIDIA graphics card GeForce GTX Titan X used in our experiments.}
\label{fig:GPUim1} 
\end{figure}
A Graphics Processing Unit (GPU) is a chip architecture mainly desing for imaging and gaming. It differs from general-purpose CPU's in that they can handle larger amounts of data efficiently and faster in a parallel way, making them much more suitable for deep learning applications. It has extended its usage into machine learning area because companies like NVIDIA have develop GPU's specially optimized for neural networks. Not only that, but they have also written routines (NVIDIA Deep Learning SDK) that improve and speed up common operations like convolutions, better memory management and frienlier API's for easy learning. By distributing the workload of the training process the convergence happens much faster than  by using conventional CPU's. See Figure \ref{fig:GPUim1}
\section{Datasets}
It is also important to mention that along with better hardware and more ingenious algorithms deep learning has achieved major sucess because in recent years more and more data is becoming available through the internet. For most of machine learning methods data is the most important asset to obtain sucessfult results. In fact, choosing the right dataset is the first step in the deep learning workflow cycle and it is, in general, much more important than the chosen learning algorithm itself. \
Since our goal is to reconstruct images, we make use of three different image datasets BioID \cite{frischholz2003bioid}, Label faces in the wild (LFW) \cite{LFWTech} and LabelMe \cite{russell2008labelme}. The first two datasets served as an initial baseline for measuring the plausibility of our approach and the last one allows for a more robust justification as explained later. All of the datasets are known in the community for being applied for different tasks like face detection, emotion recognition and image classification. Here, we propose its usage for image reconstruction from compressed samples. 
\begin{table}[tb]
\caption[Datasets for training and testing]{Original features of each dataset.}
\label{tab:datasets1}
\centering
\begin{tabular}{l*{6}{c}r}
Dataset name              & Number of images & training & testing &  Original image size& Grayscale \\
\hline
BioID   & 1521 & 1371 & 150 & 384x286 & Yes\\
LFW     & 13233 & 11933 & 1300 & 250x250 & No\\
LabelMe & 50000 & 40000 & 10000 & 256x256 & No\\
\bottomrule 
\end{tabular}  
\end{table}

All datasets differ in terms of image size, color model (RGB or Grayscale) and file format (jpg, png, bmp, etc.). In order to facilitate implementation all datasets had to be transformed into a more generic form that could meet our constrains. Namely, it is more convenient to work with grayscale images and once promissing result are obtained it can be easily extend to RGB imgaes or even video. For that reason LFW and LabelMe datasets were converted into grayscale images using rgb2gray MATLAB function. Furtheremore, having an image size that was divisible by 16 was an initial requirement and so each image was resize to $256x256$ using imresize. The actual data being used is described in table \ref{tab:datasets2}.   
\begin{table}[tb]
\caption[Transformed datasets for training and testing]{Transformed features of each dataset.}
\label{tab:datasets2}
\centering
\begin{tabular}{l*{6}{c}r}
Dataset name              & Number of images & training & testing &  Original image size& Grayscale \\
\hline
BioID   & 1521 & 1371 & 150 & 256x256 & Yes\\
LFW     & 13233 & 11933 & 1300 & 256x256 & Yes\\
LabelMe & 50000 & 40000 & 10000 & 256x256 & Yes\\
\bottomrule 
\end{tabular}  
\end{table}

Finally, in order to evaluate the performance of our trained network we introduce a set of 18 images. The images, we believe, are the most popular ones in the image processing community and serve as a baseline for many reconstruction algorithms and image processing tasks. They also help compared our results against the ones reported by other researchers. The evaluation set is shown in figure \ref{fig:EVALim1}
\begin{figure}[tb] 
\centering 
\includegraphics[scale=0.7]{EVAL1.png} 
\caption[Evaluation images]{From Top Left to Right Bottom:'fingerprint','couple','boats','flintstones','peppers','woman',
'butterfly','bridge','houses','house','mandril','parrot','montage','foreman','man','barbara','lena','cameraman'}
\label{fig:EVALim1} 
\end{figure}

\subsection{Preprocessing of the images}
Even though grayscale images can be easily interpreted as a $2D$ matrix, CNN's expect as an input a $3D$ tensor, chanpter 2.2.2, complying with the convention depth, height and width. Consequently, extra preprocessing is carried out to generate the data set that is used as the final input of the network for training. Moreover, we make use of a technique called Block Compressed Sensing (BCS). BCS allows to divide the image into small blocks of size $B \times B$ with $B=16$ and then compress them by using the same sensing matrix $\Phi$. It has the main advantage of having a small sensing matrix that can be easily extended to process high-dimensional images and making the overall process faster \cite{gan2007block, fowler2012block}. Another important parameter is the subrate or subsampling size $M$. That is, how much information we would like to sense. For most of our experiments the we choose $M=16$, we also show results with different $M$ values, yielding a subrate of $1/16$. For an image of size $256 \times 256$ that means there are $65536$ possible samples (original pixel values), by using a subrate of $1/16$ we will only be taking $4096$ measurements from which we weill reconstruct the image.          \
\\\\
To demosntrate the process, consider an image randomly taken from one of the datasets. It has a size of $N \times N$ with $N=256$. First, it is converted into  blocks of size $B$ by using MALAB command im2col. Second, the image is multiplied with the sensing matrix $\Phi$ to obtain the measurements. Since the size of $\Phi$ is $M \times N $ and the image size is $N \times N$ the product will have size $M \times N$. At last, the measurements are reshaped into a 3D tensor of size $M \times N \times N$. A pictorical representation of the process can be seen in Figure \ref{fig:PREPROim1}. After preprocessing each image individually, the data used for training and testing is a 4D tensor and each dimmesion is interpreted as follow: $(Number \enspace of \enspace images, \enspace size \enspace of \enspace M ,\enspace size \enspace of \enspace B , \enspace size \enspace of \enspace B)$.  
\begin{figure}[tb] 
\centering 
\includegraphics[scale=0.8]{PREPRO1.png} 
\caption[Preprocessing of images]{Example of image preprocessing  with $B=16$, $M=16$ and $N=256$.}
\label{fig:PREPROim1} 
\end{figure}

\subsection{Network architectures for CS recovery}
The architecture of the network refers to the other in which different layers are arrenged and interconnected. Throughout the development of the thesis different architectures were heuristically tested and in the following we present the ones the achieved better performance.
\subsubsection{Alpha CNN network}
This is the first network we propose to recover images from compressed samples is in Figure \ref{fig:ARCNNim1}. It is compossed of an input layer and an output layer. The input layer receives measurements of size $(size \enspace of \enspace M ,\enspace size \enspace of \enspace B , \enspace size \enspace of \enspace B)$, with a kernel size $k=128$ and followed by ReLu. The output layer receives as input the output of the previous layer that is $(size \enspace of \enspace k \enspace in \enspace previous \enspace layer ,\enspace size \enspace of \enspace B , \enspace size \enspace of \enspace B)$ and a kernel $k=256$. The ouput layer is designend so that it matches the size of the original image.  
\begin{figure}[tb] 
\centering 
\includegraphics[scale=0.5]{ARCNN1.png} 
\caption[Alpha CNN architecture for recovery ]{Depiction of the alpha CNN architecture.}
\label{fig:ARCNNim1} 
\end{figure}

\subsubsection{Beta CNN network}
During the development of this thesis several attemps for reconstructing images using deep learning were published \cite{kulkarni2016reconnet,mousavi2015deep,iliadis2016deep,iliadis2016deepbinarymask,adler2016deep}. Interestingly, Kulkarni \textit{et al}. \cite{kulkarni2016reconnet} also addressed the problem of CS recovery using a CNN. The other approaches are very similar because they are based on an auto-encoder.  Even more, they do not compressed the images using iid Gaussian matrices. Rather, they learn the sensing matrix $\Phi$ in the first layer. Other interesting findings were made by Kulkarni \textit{et al}. \cite{kulkarni2016reconnet}. They introduced a new parameters that controls the redundancy of the network. Redundancy refers to how large the network will be in relation with the size of the original image. For example, a redundancy factor of 8, would create a reconstruction layer of size $redundancy \times N$. The value 8 turned out to yield better reconstruction performance according to their tests.\\\\
Based on the previous concepts and proved capabilities of the alpha network we proposed another nework sketched in Figure %\ref      


\subsection{Training CNN for recover}

\subsection{Postprocessing of reconstructed images}

\subsection{Evaluation metrics}

  
